0000 
0110 
1001
0010
1010
0001
1000
0110
0000
0100
1000
0110
0100
1000
0010
0100

XORROR1 S = 0010

XORRO2 S =  0011

Response = 0
============================

2.9105443427999944 6.189227954799992 88156.0 0.9321249999999999

3.1133923138000683 7.429136546200016 87706.0 0.9197749999999999

1.4055762991999472 6.472391509600129 83876.0 0.9445

===========================
CS971

1.502902739995625 2.2300484200008213 88152.0 0.9321249999999999


CS983
4.084645380004076 2.193555680004647 84118.0 0.9378949999999999
=============

1.2363050096000052 6.360938097799999 83876.0 0.9445

1.3912564874000055 6.249904980399992 84716.0 0.947425


========
fit_intercept=True - decreases the accuracy
================
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=0.01)
2.0125108240000147 6.699569712799985 88996.0 0.825225
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=1)
2.2023023144000033 7.261396257000001 88156.0 0.9390750000000001
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=10)
2.846358982999982 6.852763996199974 88156.0 0.94945
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=11)
3.1303430156000105 7.039806393000072 88156.0 0.94955
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=20)
3.3007079377999617 6.654927636799949 88156.0 0.9496499999999999
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=40)
3.20083408240007 6.624912874600023 88156.0 0.949975
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=60)
3.729819408999947 6.229424281200044 88156.0 0.9496499999999999
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=100)
3.4016365134000353 6.707200896399991 88156.0 0.9492749999999999

===============

clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=10, tol=1e-10)
4.239220157800037 6.45599754459995 88156.0 0.949475


clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=11, tol=1e-10)
3.813806633999866 7.09931211159992 88156.0 0.94955
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=11, tol=1e-4)
2.130466009800057 7.491350201799923 88156.0 0.94955


clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=11, tol=1e-1)
1.0446536763998666 7.026190121400032 88156.0 0.790625
============
clf=LogisticRegression(max_iter=10000, solver='newton-cg', C=11, tol=1e-4,penalty='l2')
3.14519290979988 6.486940754199895 88156.0 0.94955

clf=LogisticRegression(max_iter=10000, solver='liblinear', C=10, tol=1e-4,penalty='l2')
1.3102126592000787 7.1384906877998215 88156.0 0.9484999999999999


clf=LogisticRegression(max_iter=10000, solver='liblinear', C=11, tol=1e-4,penalty='l2')*****
0.9914572810000208 7.048575445200004 88156.0 0.9487500000000001
 
clf=LogisticRegression(max_iter=10000, solver='liblinear', C=11, tol=1e-4,penalty='l1')
3.7356729968000764 6.412336455400146 88156.0 0.9476600000000002
   
clf=LogisticRegression(max_iter=10000, solver='liblinear', C=11, tol=1e-10,penalty='l2')
1.1685585180000089 7.284882889199889 88156.0 0.9487

clf=LogisticRegression(max_iter=10000, solver='liblinear', C=40, tol=1e-4,penalty='l2')
1.5100987823999277 6.74268217459994 88156.0 0.94945

clf=LogisticRegression(max_iter=10000, solver='liblinear', C=1, tol=1e-4,penalty='l2')
 0.7836511866000364 7.345262839800034 88156.0 0.9403749999999998
 
=============
If you prioritize accuracy, go for C=40 with the newton-cg solver.
If you need a balance of speed and accuracy, consider C=11 with the liblinear solver for faster training while still achieving high accuracy.
Final Recommendation
Given your goal of avoiding overfitting while maintaining high accuracy, I recommend the following options:

For Highest Accuracy:

C=40, solver='newton-cg' (94.99% accuracy)
For Best Performance with Speed:

C=11, solver='liblinear', tol=1e-4, penalty='l2' (94.88% accuracy)
For Good Balance:

C=10, solver='newton-cg' (94.95% accuracy)
Conclusion
Choose C=40 for optimal accuracy, or consider the liblinear solver with C=11 for faster performance while still achieving high accuracy. 
Monitor validation performance to ensure your choice generalizes well to unseen data. 
===============
clf=LogisticRegression(max_iter=10000, C=0.01)
1.6619612469999994 7.036595888600004 88992.0 0.825175
clf=LogisticRegression(max_iter=10000, C=1)
2.1194421102000205 6.960793899999999 88152.0 0.939325
clf=LogisticRegression(max_iter=10000, C=10)
3.0364275532001104 6.494699841800047 88152.0 0.9498

clf=LogisticRegression(max_iter=10000, C=11)
2.8936114139999516 6.509436231000018 88152.0 0.94985

=================
clf=LogisticRegression(max_iter=10000, tol=1e-10)
3.0170075201999964 5.907416339600007 88992.0 0.939175


clf=LogisticRegression(max_iter=10000, tol=1e-3)
1.7349334075999878 7.106920074800018 88992.0 0.938775

clf=LogisticRegression(max_iter=10000, tol=1e-4)
2.558908140799974 6.835501674799991 88992.0 0.939325

clf=LogisticRegression(max_iter=10000, tol=1e-1)
1.435498751199998 6.876757038999972 88992.0 0.7798
 
 
clf=LogisticRegression(max_iter=10000, tol=1e1)
1.2304760762 7.390591038600019 88992.0 0.46299999999999997
==================

clf = LinearSVC(max_iter=10000, C=0.01) 
1.0418885699999918 7.182633459000135 84716.0 0.903975

clf = LinearSVC(max_iter=10000, C=0.1)
0.9719954693999171 7.187399452400041 84716.0 0.9399999999999998

clf = LinearSVC(max_iter=10000, C=1) 
0.9099672247998569 7.388745800799915 83876.0 0.947425

clf = LinearSVC(max_iter=10000, C=10)
1.2965315207999992 6.985495421199903 83876.0 0.9445

=================
clf = LinearSVC(max_iter=10000, tol=1e-10) 
1.20513244699996 7.205967684199822 84716.0 0.947425

clf = LinearSVC(max_iter=10000, tol=1e-4)
0.9596972766000362 7.101524155999959 84716.0 0.947425

clf = LinearSVC(max_iter=10000, tol=1e1)
1.2211389825999504 6.716436382000029 84716.0 0.947425

clf = LinearSVC(max_iter=10000, tol=1e10) 
1.373592323199955 7.140198136199979 84716.0 0.947425
=============
clf = LinearSVC(max_iter=10000, penalty='l2')
1.395721565399981 6.855036144000041 84716.0 0.947425



  